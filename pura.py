# -*- coding: utf-8 -*-
"""pura

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TTtW3wmDUC1F4ASCInrg76Vg7RGbTlJS
"""

import torch
import torch.nn as nn
import numpy as np
import pickle
import pandas
import torch.optim

from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

def trainData(arr, y, batch = 64, istrain = True):
    class txtData(torch.utils.data.Dataset):
        def __init__(self, arr, y, istrain, transform):
            self.arr = arr
            self.labels = y
            self.istrain = istrain
            self.transform = transform
                
        def __len__(self):
            return len(self.labels)
            
        def __getitem__(self, idx):
            data = self.arr[idx]
            label = self.labels[idx]
                
            if self.istrain:
                return data, label
         
    data_transform = transforms.Compose([
                transforms.ToTensor()])                 
    dataset = txtData(arr, y, istrain=istrain, transform=data_transform)     
    dataloader = DataLoader(dataset, batch_size = batch, shuffle=False, num_workers=0)  
        
    return dataloader

class TensorDot(nn.Module):
    def __init__(self):
        super(TensorDot, self).__init__()
        
    def forward(self, x, y):
        final = []
        n = int(x.shape[0])
        for f in range(n):
            c = []
            for i in y[f]:
                summ = 0
                for j in x[f]:
                    summ = summ + i*j
                c.append(summ)
            final.append(c)
        return np.array(final, dtype='float32') 
    
class mainNet(nn.Module):
    def __init__(self):
        super(mainNet, self).__init__()
        
        self.lstm = nn.LSTM(input_size=768, hidden_size=100, num_layers=2, batch_first=True)
        self.fc = nn.Sequential(
                nn.Linear(115, 64),
                nn.ReLU()
                )
        self.tdot = TensorDot()
        self.fcf = nn.Sequential(
                nn.Linear(279, 6),
                nn.ReLU()
                )
        self.softmax = nn.Softmax()
        
    def forward(self, main, aux, cred):
        x, _ = self.lstm(main)
        x = x[:, -1:]
        x = x.squeeze()
        
        z = self.fc(aux)
        y = self.tdot(x, cred)
        
        y = torch.Tensor(y)
        y = y.to(device)
        
        out = torch.cat((x, y, z), 1)
        out = self.fcf(out)
        fin_out = self.softmax(out)
        
        return fin_out

device = torch.device("cpu")

with open("X_train.txt", "rb") as fp: 
    a = pickle.load(fp)
with open("X_train_meta.txt", "rb") as fp: 
    b = pickle.load(fp)
with open("X_train_cred.txt", "rb") as fp: 
    c = pickle.load(fp)
    
with open("X_val.txt", "rb") as fp: 
    d = pickle.load(fp)
with open("X_val_cred.txt", "rb") as fp: 
    e = pickle.load(fp)
with open("X_val_meta.txt", "rb") as fp: 
    f = pickle.load(fp)

a = a.tolist()
d = d.tolist()

with open("Y_train.txt", "rb") as fp:
    y_train = pickle.load(fp)
with open("Y_val.txt", "rb") as fp:
    y_val = pickle.load(fp)

y_train = y_train.tolist()
y_val = y_val.tolist()

tmain = trainData(a, y_train, 2000)
taux = trainData(b, y_train, 2000)
tcred = trainData(c, y_train, 2000)

vmain = trainData(d, y_val, 1)
vaux = trainData(e, y_val, 1)
vcred = trainData(f, y_val, 1)

model = mainNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
loss_function = torch.nn.CrossEntropyLoss()

num_epochs = 1
losses = []

for epoch in range(num_epochs):
    train_loss = []
    valid_loss = []
    
    model.train()
    
    main_iter = iter(tmain)
    aux_iter = iter(taux)
    cred_iter = iter(tcred)
    length = len(taux)
    i = 0
    while (i < length):
        mainiter = main_iter.next()
        auxiter = aux_iter.next()
            
        crediter = aux_iter.next()
            
        main, label = mainiter
        aux, _ = auxiter
        print(aux)
        cred, _ = crediter
        
        optimizer.zero_grad()
            
        main = torch.Tensor.float(main)
        aux = torch.Tensor.float(aux)
        cred = torch.Tensor.float(cred)
            
        output = model(main, aux, cred)
            
        loss = loss_function(output, label)
            
        loss.backward()
        optimizer.step()
        print("Train:")
        i+=1
        print(epoch, i, loss.item())
        train_loss.append(loss.item())
    model.eval()
    
    main_iter_v = iter(vmain)
    aux_iter_v = iter(vaux)
    cred_iter_v = iter(vcred)
    length = len(vaux)
    i = 0
    while (i < length):
        mainiter = main_iter_v.next()
        auxiter = aux_iter_v.next()
        crediter = aux_iter_v.next()
        
        main, label = mainiter
        aux, _ = auxiter
        cred, _ = crediter
        
        main = torch.Tensor.float(main)
        aux = torch.Tensor.float(aux)
        cred = torch.Tensor.float(cred)
        
        output = model(main, aux, cred)

        loss = loss_function(output, label)
        print("Validation:")
        i+=1
        print(epoch, i, loss.item())
        valid_loss.append(loss.item())
        
    losses.append([train_loss, valid_loss])